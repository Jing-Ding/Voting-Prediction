{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import Lasso, RidgeCV, LassoCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier, XGBRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Guth Classification</th>\n",
       "      <th>Strict Classification</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HRMONTH</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HRYEAR4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HURESPLI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>too many variables</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUFINAL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Variable  Guth Classification  Strict Classification  Unnamed: 3  \\\n",
       "0        id                  4.0                    4.0         NaN   \n",
       "1   HRMONTH                  4.0                    4.0         NaN   \n",
       "2   HRYEAR4                  4.0                    4.0         NaN   \n",
       "3  HURESPLI                  3.0                    4.0         NaN   \n",
       "4   HUFINAL                  2.0                    4.0         NaN   \n",
       "\n",
       "                Notes  Unnamed: 5 Type  \n",
       "0                 NaN         NaN  NaN  \n",
       "1                 NaN         NaN  NaN  \n",
       "2                 NaN         NaN  NaN  \n",
       "3  too many variables         NaN  NaN  \n",
       "4                 NaN         NaN  NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_card = pd.read_csv('data/variable_classification2.csv')\n",
    "df_card.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48500, 111) (48500,) (16167, 111) (16167,) (16000, 111)\n"
     ]
    }
   ],
   "source": [
    "sig = df_card.index[df_card['Guth Classification'].isin([1, 2, 3])].tolist()\n",
    "\n",
    "train_data = np.genfromtxt('data/train_2008.csv', delimiter=',', skip_header=0)\n",
    "\n",
    "X = train_data[1:, sig]\n",
    "y = train_data[1:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, \n",
    "                                                    stratify=y, random_state=42)\n",
    "# X_train[X_train < 0] = -1\n",
    "# X_test[X_test < 0] = -1\n",
    "\n",
    "pred_data = np.genfromtxt('data/test_2008.csv', delimiter=',', skip_header=0)\n",
    "X_pred = pred_data[1:, sig]\n",
    "# X_pred[X_pred < 0] = -1\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, X_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25538144329896906 0.2553967959423517\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y_train) / len(y_train), np.sum(y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:10], y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_rf = df_card.index[df_card['Guth Classification'].isin([1, 2])].tolist()\n",
    "\n",
    "X_rf = train_data[1:, sig_rf]\n",
    "\n",
    "X_train_rf, X_test_rf, y_train, y_test = train_test_split(X_rf, y, test_size=0.25, \n",
    "                                                          stratify=y, random_state=42)\n",
    "\n",
    "X_pred_rf = pred_data[1:, sig_rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 1. 0. 0. 0.] (48500, 54)\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:10], y_test[:10], X_train_rf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "# Standardization. Fit on training set only.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_pred = scaler.transform(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_rf)\n",
    "\n",
    "X_train_rf = scaler.transform(X_train_rf)\n",
    "X_test_rf = scaler.transform(X_test_rf)\n",
    "X_pred_rf = scaler.transform(X_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "scaler.fit(X_rf)\n",
    "X_rf = scaler.transform(X_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for cross_validation\n",
    "n_folds = 5\n",
    "\n",
    "def auc_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train)\n",
    "    score = np.average(cross_val_score(model, X_train, y_train, scoring=\"roc_auc\", cv = kf))\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=[0.0001, 0.001, 0.01, 0.1, 1, 3, 10], cv=5, fit_intercept=True,\n",
       "    gcv_mode=None, normalize=False, scoring='roc_auc',\n",
       "    store_cv_values=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge1 = RidgeCV(alphas=[1e-4, 1e-3, 1e-2, 1e-1, 1, 3, 10], cv=5, scoring='roc_auc')\n",
    "ridge1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7683312925647523 0.001\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_test, ridge1.predict(X_test)), ridge1.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=[0.03, 0.05, 0.1, 0.2, 0.3, 0.5], cv=5, fit_intercept=True,\n",
       "    gcv_mode=None, normalize=False, scoring='roc_auc',\n",
       "    store_cv_values=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge2 = RidgeCV(alphas=[3e-2, 5e-2, 1e-1, 2e-1, 3e-1, 5e-1], cv=5, scoring='roc_auc')\n",
    "ridge2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7683264971297816 0.5\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_test, ridge2.predict(X_test)), ridge2.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7676877913876192"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso1 = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "\n",
    "auc_cv(lasso1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7658938472081319"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso1.fit(X_train, y_train)\n",
    "roc_auc_score(y_test, lasso1.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elastic Net Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7872970394405379"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBoost = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "\n",
    "auc_cv(GBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7903717336917991"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBoost.fit(X_train, y_train)\n",
    "roc_auc_score(y_test, GBoost.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1 = XGBRegressor(learning_rate=0.05,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective= 'binary:logistic',\n",
    "                    seed=27,\n",
    "                    gamma=0.2,\n",
    "                    max_depth=7,\n",
    "                    min_child_weight=5,\n",
    "                    n_estimators=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7898283162334581"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_cv(xgb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-48b7a134bf94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxgb1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    371\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1045\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xgb1.fit(X_train, y_train)\n",
    "roc_auc_score(y_test, xgb1.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2 = XGBRegressor(learning_rate=0.05,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective= 'binary:logistic',\n",
    "                    seed=27,\n",
    "                    gamma=0.2,\n",
    "                    max_depth=7,\n",
    "                    min_child_weight=5,\n",
    "                    n_estimators=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7880072647794094"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_cv(xgb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7936397902967398"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb2.fit(X_train, y_train)\n",
    "roc_auc_score(y_test, xgb2.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb3 = XGBRegressor(learning_rate=0.02,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective= 'binary:logistic',\n",
    "                    seed=27,\n",
    "                    gamma=0.2,\n",
    "                    max_depth=7,\n",
    "                    min_child_weight=5,\n",
    "                    n_estimators=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7918243559134321"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_cv(xgb3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb3.fit(X_train, y_train)\n",
    "roc_auc_score(y_test, xgb3.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb4 = XGBRegressor(learning_rate=0.01,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective= 'binary:logistic',\n",
    "                    seed=27,\n",
    "                    gamma=0.2,\n",
    "                    max_depth=7,\n",
    "                    min_child_weight=5,\n",
    "                    n_estimators=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7915313055479716"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_cv(xgb4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7949095644530191"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb4.fit(X_train, y_train)\n",
    "roc_auc_score(y_test, xgb4.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'objective': 'regression', 'num_leaves': 35,\n",
    "          'learning_rate': 0.01, 'n_estimators': 4500,\n",
    "          'max_bin': 100, 'bagging_fraction': 0.8,\n",
    "          'bagging_freq': 5, 'feature_fraction': 0.5,\n",
    "          'feature_fraction_seed': 9, 'bagging_seed': 9,\n",
    "          'min_data_in_leaf': 3, 'min_sum_hessian_in_leaf': 11}\n",
    "\n",
    "lgbm1 = lgb.LGBMRegressor(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7966988849510256\n"
     ]
    }
   ],
   "source": [
    "lgbm1.fit(X_train, y_train)\n",
    "lgb_pred = np.expm1(lgbm1.predict(X_test))\n",
    "print(roc_auc_score(y_test, lgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2397,  547,  195, 1658, 1023,  353,  621,  660, 6802,    0, 3358,\n",
       "       2046, 4050, 1649, 1069,  621, 2313, 7825, 6790, 5017, 2162,  650,\n",
       "        668, 3099, 2670, 2602,  725, 9464,  202, 1937, 1197,  340,    0,\n",
       "       5365, 2619, 1042,  380, 1307,  725,  314, 1068,    0, 1592, 2426,\n",
       "       2335,  279, 1922, 1043,  735,  951,  213,  428,  471,  327,  641,\n",
       "        558,  296,  274, 2767,  537,  613,  456, 3489,   40, 1658,   74,\n",
       "        145, 1135, 1318, 1045, 1238,   72,  416,  755, 5016, 4349, 1254,\n",
       "        307, 1285, 1367, 1301,  553,  277,  731,  583,  397,  470,  316,\n",
       "       2750, 1413,  186,   79,  367,  609,  128,  298, 1569, 2063,  593,\n",
       "       1416,  898,  538,  403,  837,  289,  249,  551,  484,  273,  579,\n",
       "        443])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm1.booster_.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.7828925464872372 \n",
      "\n",
      "#### Best params ####\n",
      "\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'n_estimators': 1500}\n",
      "[mean: 0.77500, std: 0.00163, params: {'max_depth': 10, 'min_samples_leaf': 1, 'n_estimators': 1500}, mean: 0.77505, std: 0.00142, params: {'max_depth': 10, 'min_samples_leaf': 2, 'n_estimators': 1500}, mean: 0.78203, std: 0.00194, params: {'max_depth': 15, 'min_samples_leaf': 1, 'n_estimators': 1500}, mean: 0.78180, std: 0.00159, params: {'max_depth': 15, 'min_samples_leaf': 2, 'n_estimators': 1500}, mean: 0.78278, std: 0.00229, params: {'max_depth': 20, 'min_samples_leaf': 1, 'n_estimators': 1500}, mean: 0.78289, std: 0.00188, params: {'max_depth': 20, 'min_samples_leaf': 2, 'n_estimators': 1500}, mean: 0.78160, std: 0.00222, params: {'max_depth': 25, 'min_samples_leaf': 1, 'n_estimators': 1500}, mean: 0.78285, std: 0.00178, params: {'max_depth': 25, 'min_samples_leaf': 2, 'n_estimators': 1500}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "parameters_rfc = { \n",
    "    'n_estimators': [1500],\n",
    "    'max_depth': [10, 15, 20, 25],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rfc_grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42,\n",
    "                                                               n_jobs=-1),\n",
    "                               param_grid=parameters_rfc,\n",
    "                               cv=5, \n",
    "                               scoring='roc_auc',\n",
    "                               return_train_score=True)\n",
    "\n",
    "rfc_grid_search.fit(X_train_rf, y_train)\n",
    "\n",
    "print('Best score: {} '.format(rfc_grid_search.best_score_))\n",
    "print('\\n#### Best params ####\\n')\n",
    "print(rfc_grid_search.best_params_)\n",
    "print(rfc_grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7882801177236"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf1 = RandomForestClassifier(n_estimators=1500, max_depth=30, min_samples_leaf=2, \n",
    "                            criterion='entropy', n_jobs=-1)\n",
    "rf1.fit(X_train_rf, y_train)\n",
    "roc_auc_score(y_test, rf1.predict_proba(X_test_rf)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997551066624777"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_train, rf1.predict_proba(X_train_rf)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7883685597046344"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf2 = RandomForestClassifier(n_estimators=1200, max_depth=35, min_samples_leaf=2, \n",
    "                            criterion='entropy', n_jobs=-1)\n",
    "rf2.fit(X_train_rf, y_train)\n",
    "roc_auc_score(y_test, rf2.predict_proba(X_test_rf)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9981069788771318"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_train, rf2.predict_proba(X_train_rf)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7870892090281155"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf3 = RandomForestClassifier(n_estimators=500, max_depth=30, min_samples_leaf=2, \n",
    "                            criterion='entropy', n_jobs=-1)\n",
    "rf3.fit(X_train_rf, y_train)\n",
    "roc_auc_score(y_test, rf3.predict_proba(X_test_rf)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9964203535691708"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_train, rf3.predict_proba(X_train_rf)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_72 (Dense)             (None, 200)               22400     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 400)               80400     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 600)               240600    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 400)               240400    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 100)               40100     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 624,001\n",
      "Trainable params: 624,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48500 samples, validate on 16167 samples\n",
      "Epoch 1/20\n",
      "48500/48500 [==============================] - 15s 302us/step - loss: 0.5137 - acc: 0.7535 - val_loss: 0.4880 - val_acc: 0.7664\n",
      "Epoch 2/20\n",
      "48500/48500 [==============================] - 13s 258us/step - loss: 0.4893 - acc: 0.7645 - val_loss: 0.4813 - val_acc: 0.7729\n",
      "Epoch 3/20\n",
      "48500/48500 [==============================] - 12s 254us/step - loss: 0.4819 - acc: 0.7702 - val_loss: 0.4781 - val_acc: 0.7722\n",
      "Epoch 4/20\n",
      "48500/48500 [==============================] - 12s 252us/step - loss: 0.4781 - acc: 0.7732 - val_loss: 0.4777 - val_acc: 0.7731\n",
      "Epoch 5/20\n",
      "48500/48500 [==============================] - 13s 259us/step - loss: 0.4754 - acc: 0.7737 - val_loss: 0.4778 - val_acc: 0.7739\n",
      "Epoch 6/20\n",
      "48500/48500 [==============================] - 14s 283us/step - loss: 0.4736 - acc: 0.7753 - val_loss: 0.4762 - val_acc: 0.7746\n",
      "Epoch 7/20\n",
      "48500/48500 [==============================] - 14s 283us/step - loss: 0.4707 - acc: 0.7784 - val_loss: 0.4765 - val_acc: 0.7745\n",
      "Epoch 8/20\n",
      "48500/48500 [==============================] - 13s 266us/step - loss: 0.4699 - acc: 0.7778 - val_loss: 0.4755 - val_acc: 0.7724\n",
      "Epoch 9/20\n",
      "48500/48500 [==============================] - 13s 263us/step - loss: 0.4669 - acc: 0.7804 - val_loss: 0.4776 - val_acc: 0.7724\n",
      "Epoch 10/20\n",
      "48500/48500 [==============================] - 13s 266us/step - loss: 0.4663 - acc: 0.7817 - val_loss: 0.4763 - val_acc: 0.7741\n",
      "Epoch 11/20\n",
      "48500/48500 [==============================] - 13s 271us/step - loss: 0.4634 - acc: 0.7823 - val_loss: 0.4791 - val_acc: 0.7753\n",
      "Epoch 12/20\n",
      "48500/48500 [==============================] - 13s 262us/step - loss: 0.4619 - acc: 0.7824 - val_loss: 0.4805 - val_acc: 0.7753\n",
      "Epoch 13/20\n",
      "48500/48500 [==============================] - 13s 277us/step - loss: 0.4620 - acc: 0.7833 - val_loss: 0.4777 - val_acc: 0.7755\n",
      "Epoch 14/20\n",
      "48500/48500 [==============================] - 12s 254us/step - loss: 0.4584 - acc: 0.7856 - val_loss: 0.4805 - val_acc: 0.7739\n",
      "Epoch 15/20\n",
      "48500/48500 [==============================] - 13s 266us/step - loss: 0.4566 - acc: 0.7876 - val_loss: 0.4754 - val_acc: 0.7747\n",
      "Epoch 16/20\n",
      "48500/48500 [==============================] - 13s 267us/step - loss: 0.4558 - acc: 0.7880 - val_loss: 0.4750 - val_acc: 0.7749\n",
      "Epoch 17/20\n",
      "48500/48500 [==============================] - 13s 261us/step - loss: 0.4542 - acc: 0.7880 - val_loss: 0.4752 - val_acc: 0.7753\n",
      "Epoch 18/20\n",
      "48500/48500 [==============================] - 13s 262us/step - loss: 0.4510 - acc: 0.7902 - val_loss: 0.4762 - val_acc: 0.7760\n",
      "Epoch 19/20\n",
      "48500/48500 [==============================] - 12s 258us/step - loss: 0.4498 - acc: 0.7909 - val_loss: 0.4767 - val_acc: 0.7727\n",
      "Epoch 20/20\n",
      "48500/48500 [==============================] - 13s 259us/step - loss: 0.4477 - acc: 0.7935 - val_loss: 0.4794 - val_acc: 0.7742\n",
      "0.76905896525055\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=X_train.shape[1]))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.2))\n",
    "          \n",
    "model.add(Dense(400))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(600))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(400))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "adam = optimizers.Adam(lr=0.0001, decay=0.000001)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=20,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "probs = model.predict(X_test)\n",
    "print(roc_auc_score(y_test, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stacking with a Meta model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "i = 0\n",
    "\n",
    "rf0_pred = np.empty_like(y_train)\n",
    "rf1_pred = np.empty_like(y_train)\n",
    "xgb0_pred = np.empty_like(y_train)\n",
    "xgb1_pred = np.empty_like(y_train)\n",
    "lgbm0_pred = np.empty_like(y_train)\n",
    "lgbm1_pred = np.empty_like(y_train)\n",
    "GBoost0_pred = np.empty_like(y_train)\n",
    "\n",
    "for train, val in cv.split(X_train, y_train):\n",
    "    i += 1\n",
    "    print('Fold %d' % i)\n",
    "    \n",
    "    rf0 = RandomForestClassifier(n_estimators=1500, max_depth=30, min_samples_leaf=2, \n",
    "                                criterion='entropy', n_jobs=-1)\n",
    "    rf0.fit(X_train_rf[train], y_train[train])\n",
    "    rf0_pred[val] = rf0.predict_proba(X_train_rf[val])[:, 1]\n",
    "    \n",
    "    rf1 = RandomForestClassifier(n_estimators=1200, max_depth=35, min_samples_leaf=2, \n",
    "                                criterion='entropy', n_jobs=-1)\n",
    "    rf1.fit(X_train_rf[train], y_train[train])\n",
    "    rf1_pred[val] = rf1.predict_proba(X_train_rf[val])[:, 1]\n",
    "\n",
    "    xgb0 = XGBRegressor(learning_rate=0.05,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective= 'binary:logistic',\n",
    "                    seed=27,\n",
    "                    gamma=0.2,\n",
    "                    max_depth=6,\n",
    "                    min_child_weight=5,\n",
    "                    n_estimators=300)\n",
    "    xgb0.fit(X_train[train], y_train[train])\n",
    "    xgb0_pred[val] = xgb0.predict(X_train[val])\n",
    "    \n",
    "    xgb1 = XGBRegressor(learning_rate=0.02,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective= 'binary:logistic',\n",
    "                    seed=27,\n",
    "                    gamma=0.2,\n",
    "                    max_depth=7,\n",
    "                    min_child_weight=5,\n",
    "                    n_estimators=800)\n",
    "    xgb1.fit(X_train[train], y_train[train])\n",
    "    xgb1_pred[val] = xgb1.predict(X_train[val])\n",
    "    \n",
    "    lgbm0 = lgb.LGBMRegressor(objective='regression', num_leaves=20,\n",
    "                              learning_rate=0.03, n_estimators=2000,\n",
    "                              max_bin = 100, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf = 6, min_sum_hessian_in_leaf = 11)\n",
    "    lgbm0.fit(X_train[train], y_train[train])\n",
    "    lgbm0_pred[val] = np.expm1(lgbm0.predict(X_train[val]))\n",
    "    \n",
    "    lgbm1 = lgb.LGBMRegressor(objective='regression', num_leaves=50,\n",
    "                              learning_rate=0.01, n_estimators=3000,\n",
    "                              max_bin = 100, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf = 3, min_sum_hessian_in_leaf = 11)\n",
    "    lgbm1.fit(X_train[train], y_train[train])\n",
    "    lgbm1_pred[val] = np.expm1(lgbm1.predict(X_train[val]))\n",
    "    \n",
    "    GBoost0 = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05,\n",
    "                                       max_depth=4, max_features='sqrt',\n",
    "                                       min_samples_leaf=15, min_samples_split=10, \n",
    "                                       loss='huber', random_state =5)\n",
    "    GBoost0.fit(X_train[train], y_train[train])\n",
    "    GBoost0_pred[val] = GBoost0.predict(X_train[val])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stack = np.stack((rf0_pred, rf1_pred, xgb0_pred, xgb1_pred, \n",
    "                          lgbm0_pred, lgbm1_pred, GBoost0_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('robustscaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "       with_scaling=True)), ('lasso', Lasso(alpha=0.0005, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=1,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False))])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso0 = make_pipeline(RobustScaler(), Lasso(alpha = 0.0005, random_state=1))\n",
    "lasso0.fit(X_train_stack, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf0 = RandomForestClassifier(n_estimators=1500, max_depth=30, min_samples_leaf=2, \n",
    "                            criterion='entropy', n_jobs=-1)\n",
    "rf0.fit(X_train_rf, y_train)\n",
    "rf0_test_pred = rf0.predict_proba(X_test_rf)[:, 1]\n",
    "\n",
    "rf1 = RandomForestClassifier(n_estimators=1200, max_depth=35, min_samples_leaf=2, \n",
    "                            criterion='entropy', n_jobs=-1)\n",
    "rf1.fit(X_train_rf, y_train)\n",
    "rf1_test_pred = rf1.predict_proba(X_test_rf)[:, 1]\n",
    "\n",
    "xgb0 = XGBRegressor(learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                objective= 'binary:logistic',\n",
    "                seed=27,\n",
    "                gamma=0.2,\n",
    "                max_depth=6,\n",
    "                min_child_weight=5,\n",
    "                n_estimators=300)\n",
    "xgb0.fit(X_train, y_train)\n",
    "xgb0_test_pred = xgb0.predict(X_test)\n",
    "\n",
    "xgb1 = XGBRegressor(learning_rate=0.02,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective= 'binary:logistic',\n",
    "                    seed=27,\n",
    "                    gamma=0.2,\n",
    "                    max_depth=7,\n",
    "                    min_child_weight=5,\n",
    "                    n_estimators=800)\n",
    "xgb1.fit(X_train, y_train)\n",
    "xgb1_test_pred = xgb1.predict(X_test)\n",
    "\n",
    "lgbm0 = lgb.LGBMRegressor(objective='regression', num_leaves=20,\n",
    "                          learning_rate=0.03, n_estimators=2000,\n",
    "                          max_bin = 100, bagging_fraction = 0.8,\n",
    "                          bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                          feature_fraction_seed=9, bagging_seed=9,\n",
    "                          min_data_in_leaf = 6, min_sum_hessian_in_leaf = 11)\n",
    "lgbm0.fit(X_train, y_train)\n",
    "lgbm0_test_pred = np.expm1(lgbm0.predict(X_test))\n",
    "\n",
    "lgbm1 = lgb.LGBMRegressor(objective='regression', num_leaves=50,\n",
    "                          learning_rate=0.01, n_estimators=3000,\n",
    "                          max_bin = 100, bagging_fraction = 0.8,\n",
    "                          bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                          feature_fraction_seed=9, bagging_seed=9,\n",
    "                          min_data_in_leaf = 3, min_sum_hessian_in_leaf = 11)\n",
    "lgbm1.fit(X_train, y_train)\n",
    "lgbm1_test_pred = np.expm1(lgbm1.predict(X_test))\n",
    "\n",
    "GBoost0 = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "GBoost0.fit(X_train, y_train)\n",
    "GBoost0_test_pred = GBoost0.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7956432051683117\n",
      "0.79820457145253\n"
     ]
    }
   ],
   "source": [
    "X_test_stack = np.stack((rf0_test_pred, rf1_test_pred, xgb0_test_pred, \n",
    "                         xgb1_test_pred, lgbm0_test_pred, lgbm1_test_pred, GBoost0_test_pred), axis=-1)\n",
    "\n",
    "print(roc_auc_score(y_train, lasso0.predict(X_train_stack)))\n",
    "print(roc_auc_score(y_test, lasso0.predict(X_test_stack)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7956268361341461 1\n",
      "0.798283577744505\n"
     ]
    }
   ],
   "source": [
    "ridge1 = RidgeCV(alphas=[1e-2, 1e-1, 1], cv=5, scoring='roc_auc')\n",
    "ridge1.fit(X_train_stack, y_train)\n",
    "print(roc_auc_score(y_train, ridge1.predict(X_train_stack)), ridge1.alpha_)\n",
    "print(roc_auc_score(y_test, ridge1.predict(X_test_stack)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05427664 0.51473095 0.43966818 0.15080389 0.21254166 0.76821544\n",
      " 0.50763219 0.22587325 0.12323257 0.16563487] [0.02700327 0.45207655 0.40617937 0.20551553 0.21792324 0.8051366\n",
      " 0.48895347 0.1553283  0.0484005  0.1284436 ] [0.02777799 0.35300177 0.4384511  0.18335803 0.19940907 0.829139\n",
      " 0.5471693  0.12741223 0.06295323 0.13642126] [-0.01913085  0.57280493  0.55488735  0.15451071  0.26361533  1.14150328\n",
      "  0.61747328  0.1452715   0.06962452  0.18799581] [-0.00747068  0.39173927  0.31021518  0.11178373  0.20924724  0.76607215\n",
      "  0.51257632  0.13307725  0.02298572  0.1407127 ]\n"
     ]
    }
   ],
   "source": [
    "print(rf1_test_pred[:10], xgb0_test_pred[:10], xgb1_test_pred[:10], \n",
    "      lgbm0_test_pred[:10], GBoost0_test_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7956517742079124 0.0001\n",
      "0.7982117739614496\n"
     ]
    }
   ],
   "source": [
    "lasso1 = LassoCV(alphas=[1e-4, 1e-3, 1e-2, 1e-1, 1, 2, 3, 5, 10], cv=5)\n",
    "lasso1.fit(X_train_stack, y_train)\n",
    "print(roc_auc_score(y_train, lasso1.predict(X_train_stack)), lasso1.alpha_)\n",
    "print(roc_auc_score(y_test, lasso1.predict(X_test_stack)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14142824,  0.22483611,  0.06444748,  0.35203983,  0.05046318,\n",
       "        0.1752807 , -0.05374918])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.30326831, 0.18259294, 0.35509964, 0.13831325,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(roc_auc_score(y_train, rf0.predict_proba(X_train_rf)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "i = 0\n",
    "\n",
    "rf0_pred = np.empty_like(y)\n",
    "rf1_pred = np.empty_like(y)\n",
    "xgb0_pred = np.empty_like(y)\n",
    "xgb1_pred = np.empty_like(y)\n",
    "lgbm0_pred = np.empty_like(y)\n",
    "lgbm1_pred = np.empty_like(y)\n",
    "lgbm2_pred = np.empty_like(y)\n",
    "GBoost0_pred = np.empty_like(y)\n",
    "\n",
    "for train, val in cv.split(X, y):\n",
    "    i += 1\n",
    "    print('Fold %d' % i)\n",
    "    \n",
    "    rf0 = RandomForestClassifier(n_estimators=1500, max_depth=30, min_samples_leaf=2, \n",
    "                                criterion='entropy', n_jobs=-1)\n",
    "    rf0.fit(X_rf[train], y[train])\n",
    "    rf0_pred[val] = rf0.predict_proba(X_rf[val])[:, 1]\n",
    "    \n",
    "    rf1 = RandomForestClassifier(n_estimators=1200, max_depth=35, min_samples_leaf=2, \n",
    "                            criterion='entropy', n_jobs=-1)\n",
    "    rf1.fit(X_rf[train], y[train])\n",
    "    rf1_pred[val] = rf1.predict_proba(X_rf[val])[:, 1]\n",
    "\n",
    "    xgb0 = XGBRegressor(learning_rate=0.05,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective= 'binary:logistic',\n",
    "                    seed=27,\n",
    "                    gamma=0.2,\n",
    "                    max_depth=6,\n",
    "                    min_child_weight=5,\n",
    "                    n_estimators=300)\n",
    "    xgb0.fit(X[train], y[train])\n",
    "    xgb0_pred[val] = xgb0.predict(X[val])\n",
    "    \n",
    "    xgb1 = XGBRegressor(learning_rate=0.02,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective= 'binary:logistic',\n",
    "                    seed=27,\n",
    "                    gamma=0.2,\n",
    "                    max_depth=7,\n",
    "                    min_child_weight=5,\n",
    "                    n_estimators=800)\n",
    "    xgb1.fit(X[train], y[train])\n",
    "    xgb1_pred[val] = xgb1.predict(X[val])\n",
    "    \n",
    "    lgbm0 = lgb.LGBMRegressor(objective='regression', num_leaves=20,\n",
    "                          learning_rate=0.03, n_estimators=2000,\n",
    "                          max_bin = 100, bagging_fraction = 0.8,\n",
    "                          bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                          feature_fraction_seed=9, bagging_seed=9,\n",
    "                          min_data_in_leaf = 6, min_sum_hessian_in_leaf = 11)\n",
    "    lgbm0.fit(X[train], y[train])\n",
    "    lgbm0_pred[val] = np.expm1(lgbm0.predict(X[val]))\n",
    "    \n",
    "    lgbm1 = lgb.LGBMRegressor(objective='regression', num_leaves=50,\n",
    "                          learning_rate=0.01, n_estimators=3000,\n",
    "                          max_bin = 100, bagging_fraction = 0.8,\n",
    "                          bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                          feature_fraction_seed=9, bagging_seed=9,\n",
    "                          min_data_in_leaf = 3, min_sum_hessian_in_leaf = 11)\n",
    "    lgbm1.fit(X[train], y[train])\n",
    "    lgbm1_pred[val] = np.expm1(lgbm1.predict(X[val]))\n",
    "    \n",
    "    lgbm2 = lgb.LGBMRegressor(objective='regression', num_leaves=35,\n",
    "                              learning_rate=0.005, n_estimators=4500,\n",
    "                              max_bin = 100, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.5,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf = 3, min_sum_hessian_in_leaf = 11)\n",
    "    lgbm2.fit(X[train], y[train])\n",
    "    lgbm2_pred[val] = np.expm1(lgbm2.predict(X[val]))\n",
    "\n",
    "    GBoost0 = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05,\n",
    "                                       max_depth=4, max_features='sqrt',\n",
    "                                       min_samples_leaf=15, min_samples_split=10, \n",
    "                                       loss='huber', random_state =5)\n",
    "    GBoost0.fit(X[train], y[train])\n",
    "    GBoost0_pred[val] = GBoost0.predict(X[val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stack = np.stack((rf0_pred, rf1_pred, xgb0_pred, xgb1_pred, lgbm0_pred, \n",
    "                    lgbm1_pred, lgbm2_pred, GBoost0_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('robustscaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "       with_scaling=True)), ('lasso', Lasso(alpha=0.0005, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=1,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False))])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso0 = make_pipeline(RobustScaler(), Lasso(alpha = 0.0005, random_state=1))\n",
    "lasso0.fit(X_stack, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7970043507397631\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y, lasso0.predict(X_stack)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf0 = RandomForestClassifier(n_estimators=1500, max_depth=30, min_samples_leaf=2, \n",
    "                            criterion='entropy', n_jobs=-1)\n",
    "rf0.fit(X_rf, y)\n",
    "rf0_final_pred = rf0.predict_proba(X_pred_rf)[:, 1]\n",
    "\n",
    "rf1 = RandomForestClassifier(n_estimators=1200, max_depth=35, min_samples_leaf=2, \n",
    "                            criterion='entropy', n_jobs=-1)\n",
    "rf1.fit(X_rf, y)\n",
    "rf1_final_pred = rf1.predict_proba(X_pred_rf)[:, 1]\n",
    "\n",
    "xgb0 = XGBRegressor(learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                objective= 'binary:logistic',\n",
    "                seed=27,\n",
    "                gamma=0.2,\n",
    "                max_depth=6,\n",
    "                min_child_weight=5,\n",
    "                n_estimators=300)\n",
    "xgb0.fit(X, y)\n",
    "xgb0_final_pred = xgb0.predict(X_pred)\n",
    "\n",
    "xgb1 = XGBRegressor(learning_rate=0.02,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective= 'binary:logistic',\n",
    "                    seed=27,\n",
    "                    gamma=0.2,\n",
    "                    max_depth=7,\n",
    "                    min_child_weight=5,\n",
    "                    n_estimators=800)\n",
    "xgb1.fit(X, y)\n",
    "xgb1_final_pred = xgb1.predict(X_pred)\n",
    "\n",
    "lgbm0 = lgb.LGBMRegressor(objective='regression', num_leaves=20,\n",
    "                      learning_rate=0.03, n_estimators=2000,\n",
    "                      max_bin = 100, bagging_fraction = 0.8,\n",
    "                      bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                      feature_fraction_seed=9, bagging_seed=9,\n",
    "                      min_data_in_leaf = 6, min_sum_hessian_in_leaf = 11)\n",
    "lgbm0.fit(X, y)\n",
    "lgbm0_final_pred = np.expm1(lgbm0.predict(X_pred))\n",
    "\n",
    "lgbm1 = lgb.LGBMRegressor(objective='regression', num_leaves=50,\n",
    "                          learning_rate=0.01, n_estimators=3000,\n",
    "                          max_bin = 100, bagging_fraction = 0.8,\n",
    "                          bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                          feature_fraction_seed=9, bagging_seed=9,\n",
    "                          min_data_in_leaf = 3, min_sum_hessian_in_leaf = 11)\n",
    "lgbm1.fit(X, y)\n",
    "lgbm1_final_pred = np.expm1(lgbm1.predict(X_pred))\n",
    "\n",
    "lgbm2 = lgb.LGBMRegressor(objective='regression', num_leaves=35,\n",
    "                              learning_rate=0.005, n_estimators=4500,\n",
    "                              max_bin = 100, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.5,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf = 3, min_sum_hessian_in_leaf = 11)\n",
    "lgbm2.fit(X, y)\n",
    "lgbm2_final_pred = np.expm1(lgbm1.predict(X_pred))\n",
    "\n",
    "GBoost0 = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "GBoost0.fit(X, y)\n",
    "GBoost0_final_pred = GBoost0.predict(X_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_stack = np.stack((rf0_final_pred, rf1_final_pred, xgb0_final_pred,\n",
    "                         xgb1_final_pred, lgbm0_final_pred, lgbm1_final_pred,\n",
    "                         lgbm2_final_pred, GBoost0_final_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7970776389450361 50\n"
     ]
    }
   ],
   "source": [
    "ridge1 = RidgeCV(alphas=[1e-4, 1e-3, 1e-2, 1e-1, 5e-1, 1, 1.5, 2, 3, 5, 10, 30, 50, 100], cv=5, scoring='roc_auc')\n",
    "ridge1.fit(X_stack, y)\n",
    "print(roc_auc_score(y, ridge1.predict(X_stack)), ridge1.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ridge1.predict(X_pred_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24330442, 0.07374529, 0.12534674, 0.36491771, 0.2451156 ,\n",
       "       0.05909378, 0.48324117, 0.74069621, 0.00399904, 0.14946549,\n",
       "       0.05429185, 0.63167903, 0.25631653, 0.20353574, 0.26606331,\n",
       "       0.18893726, 0.31325271, 0.05807429, 0.22187097, 0.32127934])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21670188 0.07251642 0.11639598 0.32011635 0.21824202 0.06005744\n",
      " 0.4207333  0.63966148 0.01320735 0.13690548 0.05597409 0.54695817\n",
      " 0.22776679 0.18288438 0.23605501 0.17047049 0.27618276 0.05919051\n",
      " 0.19847583 0.28300824]\n"
     ]
    }
   ],
   "source": [
    "y_pred = (y_pred - min(y_pred))/(max(y_pred) - min(y_pred))\n",
    "print(y_pred[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17862603,  0.18255607,  0.11484109,  0.22747291,  0.0687245 ,\n",
       "        0.21593595, -0.00134703, -0.0587778 ])"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save File\n",
    "np.savetxt(\"result.csv\", np.dstack((np.arange(y_pred.size), y_pred))[0],\"%d,%f\",\n",
    "           delimiter=' ', header=\"id,target\", comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7871781712084707 0.7869574024520294 0.7923117030956115 0.7934608161550388 0.7933315026686358 0.7953673947123844 0.7945907655830209 0.7886963685537729\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y, rf0_pred), roc_auc_score(y, rf1_pred), roc_auc_score(y, xgb0_pred), \n",
    "      roc_auc_score(y, xgb1_pred), roc_auc_score(y, lgbm0_pred), roc_auc_score(y, lgbm1_pred), \n",
    "      roc_auc_score(y, lgbm2_pred), roc_auc_score(y, GBoost0_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_86 (Dense)             (None, 5)                 45        \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 10)                60        \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 2)                 22        \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 130\n",
      "Trainable params: 130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "64667/64667 [==============================] - 4s 54us/step - loss: 0.6273 - acc: 0.7446\n",
      "Epoch 2/10\n",
      "64667/64667 [==============================] - 2s 38us/step - loss: 0.5216 - acc: 0.7446\n",
      "Epoch 3/10\n",
      "64667/64667 [==============================] - 3s 41us/step - loss: 0.4858 - acc: 0.7446\n",
      "Epoch 4/10\n",
      "64667/64667 [==============================] - 2s 32us/step - loss: 0.4765 - acc: 0.7446\n",
      "Epoch 5/10\n",
      "64667/64667 [==============================] - 3s 39us/step - loss: 0.4731 - acc: 0.7446\n",
      "Epoch 6/10\n",
      "64667/64667 [==============================] - 2s 36us/step - loss: 0.4709 - acc: 0.7446\n",
      "Epoch 7/10\n",
      "64667/64667 [==============================] - 3s 39us/step - loss: 0.4692 - acc: 0.7514\n",
      "Epoch 8/10\n",
      "64667/64667 [==============================] - 2s 36us/step - loss: 0.4677 - acc: 0.7878\n",
      "Epoch 9/10\n",
      "64667/64667 [==============================] - 2s 34us/step - loss: 0.4665 - acc: 0.7882: 0s - loss: 0.4689 -\n",
      "Epoch 10/10\n",
      "64667/64667 [==============================] - 2s 35us/step - loss: 0.4655 - acc: 0.7884\n",
      "0.7912627924580539\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=X_stack.shape[1]))\n",
    "model.add(Activation('relu'))\n",
    "          \n",
    "model.add(Dense(10))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "adam = optimizers.Adam(lr=0.0001, decay=0.00001)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_stack, y, batch_size=64, epochs=10)\n",
    "\n",
    "probs = model.predict(X_stack)\n",
    "print(roc_auc_score(y, probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21934843, 0.07102345, 0.11797089, ..., 0.18478357, 0.05475999,\n",
       "       0.13407531])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
