{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import Lasso, RidgeCV, LassoCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier, XGBRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Guth Classification</th>\n",
       "      <th>Strict Classification</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HRMONTH</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HRYEAR4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HURESPLI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>too many variables</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUFINAL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Variable  Guth Classification  Strict Classification  Unnamed: 3  \\\n",
       "0        id                  4.0                    4.0         NaN   \n",
       "1   HRMONTH                  4.0                    4.0         NaN   \n",
       "2   HRYEAR4                  4.0                    4.0         NaN   \n",
       "3  HURESPLI                  3.0                    4.0         NaN   \n",
       "4   HUFINAL                  2.0                    4.0         NaN   \n",
       "\n",
       "                Notes  Unnamed: 5 Type  \n",
       "0                 NaN         NaN  NaN  \n",
       "1                 NaN         NaN  NaN  \n",
       "2                 NaN         NaN  NaN  \n",
       "3  too many variables         NaN  NaN  \n",
       "4                 NaN         NaN  NaN  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_card = pd.read_csv('data/variable_classification2.csv')\n",
    "df_card.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = df_card.index[df_card['Guth Classification'].isin([1, 2, 3])].tolist()\n",
    "\n",
    "train_data = np.genfromtxt('data/train_2008.csv', delimiter=',', skip_header=0)\n",
    "\n",
    "X = train_data[1:, sig]\n",
    "y = train_data[1:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, \n",
    "                                                    stratify=y, random_state=42)\n",
    "# X_train[X_train < 0] = -1\n",
    "# X_test[X_test < 0] = -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48500, 111) (48500,) (16167, 111) (16167,) (82820, 111)\n"
     ]
    }
   ],
   "source": [
    "pred_data = np.genfromtxt('data/test_2012.csv', delimiter=',', skip_header=0)\n",
    "X_pred = pred_data[1:, sig]\n",
    "# X_pred[X_pred < 0] = -1\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, X_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25538144329896906 0.2553967959423517\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y_train) / len(y_train), np.sum(y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:10], y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_rf = df_card.index[df_card['Guth Classification'].isin([1, 2])].tolist()\n",
    "\n",
    "X_rf = train_data[1:, sig_rf]\n",
    "\n",
    "X_train_rf, X_test_rf, y_train, y_test = train_test_split(X_rf, y, test_size=0.25, \n",
    "                                                          stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_rf = pred_data[1:, sig_rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 1. 0. 0. 0.] (48500, 54)\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:10], y_test[:10], X_train_rf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "# Standardization. Fit on training set only.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_pred = scaler.transform(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_rf)\n",
    "\n",
    "X_train_rf = scaler.transform(X_train_rf)\n",
    "X_test_rf = scaler.transform(X_test_rf)\n",
    "X_pred_rf = scaler.transform(X_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "scaler.fit(X_rf)\n",
    "X_rf = scaler.transform(X_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for cross_validation\n",
    "n_folds = 5\n",
    "\n",
    "def auc_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train)\n",
    "    score = np.average(cross_val_score(model, X_train, y_train, scoring=\"roc_auc\", cv = kf))\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "i = 0\n",
    "\n",
    "rf0_pred = np.empty_like(y)\n",
    "rf1_pred = np.empty_like(y)\n",
    "xgb0_pred = np.empty_like(y)\n",
    "xgb1_pred = np.empty_like(y)\n",
    "lgbm0_pred = np.empty_like(y)\n",
    "lgbm1_pred = np.empty_like(y)\n",
    "lgbm2_pred = np.empty_like(y)\n",
    "GBoost0_pred = np.empty_like(y)\n",
    "\n",
    "for train, val in cv.split(X, y):\n",
    "    i += 1\n",
    "    print('Fold %d' % i)\n",
    "    \n",
    "    rf0 = RandomForestClassifier(n_estimators=1500, max_depth=30, min_samples_leaf=2, \n",
    "                                criterion='entropy', n_jobs=-1)\n",
    "    rf0.fit(X_rf[train], y[train])\n",
    "    rf0_pred[val] = rf0.predict_proba(X_rf[val])[:, 1]\n",
    "    \n",
    "    rf1 = RandomForestClassifier(n_estimators=1200, max_depth=35, min_samples_leaf=2, \n",
    "                            criterion='entropy', n_jobs=-1)\n",
    "    rf1.fit(X_rf[train], y[train])\n",
    "    rf1_pred[val] = rf1.predict_proba(X_rf[val])[:, 1]\n",
    "\n",
    "    xgb0 = XGBRegressor(learning_rate=0.05,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective= 'binary:logistic',\n",
    "                    seed=27,\n",
    "                    gamma=0.2,\n",
    "                    max_depth=6,\n",
    "                    min_child_weight=5,\n",
    "                    n_estimators=300)\n",
    "    xgb0.fit(X[train], y[train])\n",
    "    xgb0_pred[val] = xgb0.predict(X[val])\n",
    "    \n",
    "    xgb1 = XGBRegressor(learning_rate=0.02,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective= 'binary:logistic',\n",
    "                    seed=27,\n",
    "                    gamma=0.2,\n",
    "                    max_depth=7,\n",
    "                    min_child_weight=5,\n",
    "                    n_estimators=800)\n",
    "    xgb1.fit(X[train], y[train])\n",
    "    xgb1_pred[val] = xgb1.predict(X[val])\n",
    "    \n",
    "    lgbm0 = lgb.LGBMRegressor(objective='regression', num_leaves=20,\n",
    "                          learning_rate=0.03, n_estimators=2000,\n",
    "                          max_bin = 100, bagging_fraction = 0.8,\n",
    "                          bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                          feature_fraction_seed=9, bagging_seed=9,\n",
    "                          min_data_in_leaf = 6, min_sum_hessian_in_leaf = 11)\n",
    "    lgbm0.fit(X[train], y[train])\n",
    "    lgbm0_pred[val] = np.expm1(lgbm0.predict(X[val]))\n",
    "    \n",
    "    lgbm1 = lgb.LGBMRegressor(objective='regression', num_leaves=50,\n",
    "                          learning_rate=0.01, n_estimators=3000,\n",
    "                          max_bin = 100, bagging_fraction = 0.8,\n",
    "                          bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                          feature_fraction_seed=9, bagging_seed=9,\n",
    "                          min_data_in_leaf = 3, min_sum_hessian_in_leaf = 11)\n",
    "    lgbm1.fit(X[train], y[train])\n",
    "    lgbm1_pred[val] = np.expm1(lgbm1.predict(X[val]))\n",
    "    \n",
    "    lgbm2 = lgb.LGBMRegressor(objective='regression', num_leaves=35,\n",
    "                              learning_rate=0.005, n_estimators=4500,\n",
    "                              max_bin = 100, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.5,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf = 3, min_sum_hessian_in_leaf = 11)\n",
    "    lgbm2.fit(X[train], y[train])\n",
    "    lgbm2_pred[val] = np.expm1(lgbm2.predict(X[val]))\n",
    "\n",
    "    GBoost0 = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05,\n",
    "                                       max_depth=4, max_features='sqrt',\n",
    "                                       min_samples_leaf=15, min_samples_split=10, \n",
    "                                       loss='huber', random_state =5)\n",
    "    GBoost0.fit(X[train], y[train])\n",
    "    GBoost0_pred[val] = GBoost0.predict(X[val])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stack = np.stack((rf0_pred, rf1_pred, xgb0_pred, xgb1_pred, lgbm0_pred, \n",
    "                    lgbm1_pred, lgbm2_pred, GBoost0_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('robustscaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "       with_scaling=True)), ('lasso', Lasso(alpha=0.0005, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=1,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False))])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso0 = make_pipeline(RobustScaler(), Lasso(alpha = 0.0005, random_state=1))\n",
    "lasso0.fit(X_stack, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7980748381462537\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y, lasso0.predict(X_stack)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf0 = RandomForestClassifier(n_estimators=1500, max_depth=30, min_samples_leaf=2, \n",
    "                            criterion='entropy', n_jobs=-1)\n",
    "rf0.fit(X_rf, y)\n",
    "rf0_final_pred = rf0.predict_proba(X_pred_rf)[:, 1]\n",
    "\n",
    "rf1 = RandomForestClassifier(n_estimators=1200, max_depth=35, min_samples_leaf=2, \n",
    "                            criterion='entropy', n_jobs=-1)\n",
    "rf1.fit(X_rf, y)\n",
    "rf1_final_pred = rf1.predict_proba(X_pred_rf)[:, 1]\n",
    "\n",
    "xgb0 = XGBRegressor(learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                objective= 'binary:logistic',\n",
    "                seed=27,\n",
    "                gamma=0.2,\n",
    "                max_depth=6,\n",
    "                min_child_weight=5,\n",
    "                n_estimators=300)\n",
    "xgb0.fit(X, y)\n",
    "xgb0_final_pred = xgb0.predict(X_pred)\n",
    "\n",
    "xgb1 = XGBRegressor(learning_rate=0.02,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective= 'binary:logistic',\n",
    "                    seed=27,\n",
    "                    gamma=0.2,\n",
    "                    max_depth=7,\n",
    "                    min_child_weight=5,\n",
    "                    n_estimators=800)\n",
    "xgb1.fit(X, y)\n",
    "xgb1_final_pred = xgb1.predict(X_pred)\n",
    "\n",
    "lgbm0 = lgb.LGBMRegressor(objective='regression', num_leaves=20,\n",
    "                      learning_rate=0.03, n_estimators=2000,\n",
    "                      max_bin = 100, bagging_fraction = 0.8,\n",
    "                      bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                      feature_fraction_seed=9, bagging_seed=9,\n",
    "                      min_data_in_leaf = 6, min_sum_hessian_in_leaf = 11)\n",
    "lgbm0.fit(X, y)\n",
    "lgbm0_final_pred = np.expm1(lgbm0.predict(X_pred))\n",
    "\n",
    "lgbm1 = lgb.LGBMRegressor(objective='regression', num_leaves=50,\n",
    "                          learning_rate=0.01, n_estimators=3000,\n",
    "                          max_bin = 100, bagging_fraction = 0.8,\n",
    "                          bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                          feature_fraction_seed=9, bagging_seed=9,\n",
    "                          min_data_in_leaf = 3, min_sum_hessian_in_leaf = 11)\n",
    "lgbm1.fit(X, y)\n",
    "lgbm1_final_pred = np.expm1(lgbm1.predict(X_pred))\n",
    "\n",
    "lgbm2 = lgb.LGBMRegressor(objective='regression', num_leaves=35,\n",
    "                              learning_rate=0.005, n_estimators=4500,\n",
    "                              max_bin = 100, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.5,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf = 3, min_sum_hessian_in_leaf = 11)\n",
    "lgbm2.fit(X, y)\n",
    "lgbm2_final_pred = np.expm1(lgbm1.predict(X_pred))\n",
    "\n",
    "GBoost0 = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "GBoost0.fit(X, y)\n",
    "GBoost0_final_pred = GBoost0.predict(X_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_stack = np.stack((rf0_final_pred, rf1_final_pred, xgb0_final_pred,\n",
    "                         xgb1_final_pred, lgbm0_final_pred, lgbm1_final_pred,\n",
    "                         lgbm2_final_pred, GBoost0_final_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7981263892516769 10\n"
     ]
    }
   ],
   "source": [
    "ridge1 = RidgeCV(alphas=[1e-4, 1e-3, 1e-2, 1e-1, 5e-1, 1, 1.5, 2, 3, 5, 10, 50, 100], cv=5, scoring='roc_auc')\n",
    "ridge1.fit(X_stack, y)\n",
    "print(roc_auc_score(y, ridge1.predict(X_stack)), ridge1.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04502395 0.15171326 0.16486158 0.19580485 0.17682159 0.5891934\n",
      " 0.56545021 0.69771015 0.06698721 0.29701103 0.20514805 0.17485968\n",
      " 0.32389987 0.10602648 0.56161191 0.25607545 0.19698244 0.10385295\n",
      " 0.0727334  0.00952961]\n",
      "[0.05031955 0.14026639 0.15135138 0.1774388  0.16143454 0.50909393\n",
      " 0.48907669 0.60058142 0.06883618 0.26276296 0.1853158  0.1597805\n",
      " 0.28543221 0.10174912 0.48584073 0.2282513  0.1784316  0.09991667\n",
      " 0.07368063 0.02039525]\n"
     ]
    }
   ],
   "source": [
    "y_pred = ridge1.predict(X_pred_stack)\n",
    "\n",
    "print(y_pred[:20])\n",
    "\n",
    "y_pred = (y_pred - min(y_pred))/(max(y_pred) - min(y_pred))\n",
    "print(y_pred[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08904679,  0.29989335,  0.14070377,  0.30691918,  0.10213684,\n",
       "        0.23775983, -0.08542721, -0.14392006])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save File\n",
    "np.savetxt(\"result_2012_2.csv\", np.dstack((np.arange(y_pred.size), y_pred))[0],\"%d,%f\",\n",
    "           delimiter=' ', header=\"id,target\", comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
